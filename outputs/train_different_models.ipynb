{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import multiprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from tqdm import notebook\n",
    "import cv2 as cv\n",
    "import pickle\n",
    "\n",
    "from lshash import LSHash\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms#, utils\n",
    "import torch.nn as nn \n",
    "from torch.optim import Adam\n",
    "\n",
    "import adabound as adabound\n",
    "\n",
    "###local scripts\n",
    "from geographic_zones import create_geo_zones, classify_geographic_zones\n",
    "from Custom_dataloader_geozones import MapDataset, Rescale, CenterCrop, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    startTime = time.time()\n",
    "        \n",
    "#     # define a seed for reproducibility\n",
    "#     seed = 5436457\n",
    "#     torch.manual_seed(seed)\n",
    "    \n",
    "    ##============Create geographic zones====================\n",
    "    ###Training set\n",
    "    trainDF = pd.read_csv(\"./data/piom_train_30k.csv\")\n",
    "    trainDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    lon_dim = 4.\n",
    "    lat_dim = 2.\n",
    "    Geo_zones = create_geo_zones(list(trainDF['llcrnrlon']), list(trainDF['llcrnrlat']), lon_dim, lat_dim)\n",
    "    n_zones = len(Geo_zones)\n",
    "    print('Number of geographic zones: ', n_zones)\n",
    "    trainDF['Geo_zone'] = trainDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)    \n",
    "#     ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(trainDF['llcrnrlon'], trainDF['llcrnrlat'], hue=trainDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", n_zones), legend=False)\n",
    "#     plt.title('Training samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "#     ##Visualizing distribution of the geographic zones\n",
    "#     plt.figure(figsize=(15, 4))\n",
    "#     trainDF['Geo_zone'].value_counts().plot(kind='bar')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xlabel('Label')\n",
    "#     plt.title('Sample distribution per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ###Validation set\n",
    "    valDF = pd.read_csv(\"./data/piom_train2_10k.csv\")\n",
    "    valDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    valDF['Geo_zone'] = valDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)\n",
    "    ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(valDF['llcrnrlon'], valDF['llcrnrlat'], hue=valDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", len(valDF['Geo_zone'].unique())), legend=False)\n",
    "#     plt.title('Validation samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ##============Create dataloader====================\n",
    "    ##Training dataloader\n",
    "    train_batch_size = 50\n",
    "    train_map_data = MapDataset(trainDF, './data/piom_train_png_30k/', transform= transforms.Compose([\n",
    "                                                Rescale(225),\n",
    "                                                CenterCrop((224,224)),\n",
    "                                                Normalize(alpha=0., beta=1.),\n",
    "                                                ToTensor(),\n",
    "                                            ]))\n",
    "    train_loader = torch.utils.data.DataLoader(train_map_data,\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=multiprocessing.cpu_count())\n",
    "    ##Validation dataloader\n",
    "    val_batch_size = 50\n",
    "    validation_map_data = MapDataset(valDF, './data/piom_train2_png_10k/', transform= transforms.Compose([\n",
    "                                                   Rescale(225),\n",
    "                                                   CenterCrop((224,224)),\n",
    "                                                   Normalize(alpha=0., beta=1.),\n",
    "                                                   ToTensor(),\n",
    "                                               ]))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_map_data,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=multiprocessing.cpu_count(),\n",
    "                                              drop_last=False)\n",
    "    print('Training batch size: ', train_batch_size)\n",
    "    print('Validation batch size: ', val_batch_size)\n",
    "    \n",
    "#     ##try data loader\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#         print(i_batch, sample_batch['image'].size())\n",
    "#         if i_batch == 1:\n",
    "#             break\n",
    "            \n",
    "    ##=============Transfer Learning from ResNet34 model for classification==========\n",
    "    model = models.resnet18(pretrained=True, progress=True)\n",
    "#     print(model)\n",
    "    ##modify last fully connected layer\n",
    "    model.fc = nn.Linear(512,n_zones) #resnet18:512,resnet34:512, resnet34:2048, resnext:2048,\n",
    "    optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "    ###defining the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ###model on GPU if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    #print(model)\n",
    "    \n",
    "    ##============Train the model on geographic zone classification - only the last FC layer====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 10:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    n_epochs = 1\n",
    "    print('============ Training phase 1 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    ##============Train the model on geographic zone classification - all layers====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 10:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "    n_epochs = 5\n",
    "    print('============ Training phase 2 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    \n",
    "#     ##========Drop last FC layer of the model for image embedding=======\n",
    "#     new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "#     ##=================save model in pth file==========================\n",
    "#     MODEL_PATH = './models/gpu_model_resnet18_4_25.pth'\n",
    "#     torch.save(new_classifier.state_dict(), MODEL_PATH)\n",
    "#     print('Model saved!!')\n",
    "    print(\"Training, time: {}h {}min {}s\".format((time.time()-startTime)//3600, \n",
    "                                                                      ((time.time()-startTime)%3600)//60,\n",
    "                                                                      (time.time()-startTime)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of geographic zones:  676\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Training batch size:  50\n",
      "Validation batch size:  50\n",
      "============ Training phase 1 - number of epochs: 1 ============\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 2.52 GiB already allocated; 81.50 MiB free; 2.92 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-913d1964084a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;31m# Track the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\site-packages\\adabound\\adabound.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 2.52 GiB already allocated; 81.50 MiB free; 2.92 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    startTime = time.time()\n",
    "        \n",
    "#     # define a seed for reproducibility\n",
    "#     seed = 5436457\n",
    "#     torch.manual_seed(seed)\n",
    "    \n",
    "    ##============Create geographic zones====================\n",
    "    ###Training set\n",
    "    trainDF = pd.read_csv(\"./data/piom_train_30k.csv\")\n",
    "    trainDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    lon_dim = 4.\n",
    "    lat_dim = 2.\n",
    "    Geo_zones = create_geo_zones(list(trainDF['llcrnrlon']), list(trainDF['llcrnrlat']), lon_dim, lat_dim)\n",
    "    n_zones = len(Geo_zones)\n",
    "    print('Number of geographic zones: ', n_zones)\n",
    "    trainDF['Geo_zone'] = trainDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)    \n",
    "#     ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(trainDF['llcrnrlon'], trainDF['llcrnrlat'], hue=trainDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", n_zones), legend=False)\n",
    "#     plt.title('Training samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "#     ##Visualizing distribution of the geographic zones\n",
    "#     plt.figure(figsize=(15, 4))\n",
    "#     trainDF['Geo_zone'].value_counts().plot(kind='bar')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xlabel('Label')\n",
    "#     plt.title('Sample distribution per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ###Validation set\n",
    "    valDF = pd.read_csv(\"./data/piom_train2_10k.csv\")\n",
    "    valDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    valDF['Geo_zone'] = valDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)\n",
    "    ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(valDF['llcrnrlon'], valDF['llcrnrlat'], hue=valDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", len(valDF['Geo_zone'].unique())), legend=False)\n",
    "#     plt.title('Validation samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ##============Create dataloader====================\n",
    "    ##Training dataloader\n",
    "    train_batch_size = 50\n",
    "    train_map_data = MapDataset(trainDF, './data/piom_train_png_30k/', transform= transforms.Compose([\n",
    "                                                Rescale(225),\n",
    "                                                CenterCrop((224,224)),\n",
    "                                                Normalize(alpha=0., beta=1.),\n",
    "                                                ToTensor(),\n",
    "                                            ]))\n",
    "    train_loader = torch.utils.data.DataLoader(train_map_data,\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=multiprocessing.cpu_count())\n",
    "    ##Validation dataloader\n",
    "    val_batch_size = 50\n",
    "    validation_map_data = MapDataset(valDF, './data/piom_train2_png_10k/', transform= transforms.Compose([\n",
    "                                                   Rescale(225),\n",
    "                                                   CenterCrop((224,224)),\n",
    "                                                   Normalize(alpha=0., beta=1.),\n",
    "                                                   ToTensor(),\n",
    "                                               ]))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_map_data,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=multiprocessing.cpu_count(),\n",
    "                                              drop_last=False)\n",
    "    print('Training batch size: ', train_batch_size)\n",
    "    print('Validation batch size: ', val_batch_size)\n",
    "    \n",
    "#     ##try data loader\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#         print(i_batch, sample_batch['image'].size())\n",
    "#         if i_batch == 1:\n",
    "#             break\n",
    "            \n",
    "    ##=============Transfer Learning from ResNet34 model for classification==========\n",
    "    model = models.vgg11(pretrained=True, progress=True)\n",
    "    ##modify last fully connected layer\n",
    "#     print(model)\n",
    "    model.fc = nn.Linear(1000,n_zones) #resnet18:512,resnet34:512, resnet34:2048, resnext:2048,\n",
    "    optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "    ###defining the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ###model on GPU if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    #print(model)\n",
    "    \n",
    "    ##============Train the model on geographic zone classification - only the last FC layer====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 3:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    n_epochs = 1\n",
    "    print('============ Training phase 1 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    ##============Train the model on geographic zone classification - all layers====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 3:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "    n_epochs = 5\n",
    "    print('============ Training phase 2 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    \n",
    "#     ##========Drop last FC layer of the model for image embedding=======\n",
    "#     new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "#     ##=================save model in pth file==========================\n",
    "#     MODEL_PATH = './models/gpu_model_resnet18_4_25.pth'\n",
    "#     torch.save(new_classifier.state_dict(), MODEL_PATH)\n",
    "#     print('Model saved!!')\n",
    "    print(\"Training, time: {}h {}min {}s\".format((time.time()-startTime)//3600, \n",
    "                                                                      ((time.time()-startTime)%3600)//60,\n",
    "                                                                      (time.time()-startTime)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    startTime = time.time()\n",
    "        \n",
    "#     # define a seed for reproducibility\n",
    "#     seed = 5436457\n",
    "#     torch.manual_seed(seed)\n",
    "    \n",
    "    ##============Create geographic zones====================\n",
    "    ###Training set\n",
    "    trainDF = pd.read_csv(\"./data/piom_train_30k.csv\")\n",
    "    trainDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    lon_dim = 4.\n",
    "    lat_dim = 2.\n",
    "    Geo_zones = create_geo_zones(list(trainDF['llcrnrlon']), list(trainDF['llcrnrlat']), lon_dim, lat_dim)\n",
    "    n_zones = len(Geo_zones)\n",
    "    print('Number of geographic zones: ', n_zones)\n",
    "    trainDF['Geo_zone'] = trainDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)    \n",
    "#     ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(trainDF['llcrnrlon'], trainDF['llcrnrlat'], hue=trainDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", n_zones), legend=False)\n",
    "#     plt.title('Training samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "#     ##Visualizing distribution of the geographic zones\n",
    "#     plt.figure(figsize=(15, 4))\n",
    "#     trainDF['Geo_zone'].value_counts().plot(kind='bar')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xlabel('Label')\n",
    "#     plt.title('Sample distribution per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ###Validation set\n",
    "    valDF = pd.read_csv(\"./data/piom_train2_10k.csv\")\n",
    "    valDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    valDF['Geo_zone'] = valDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)\n",
    "    ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(valDF['llcrnrlon'], valDF['llcrnrlat'], hue=valDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", len(valDF['Geo_zone'].unique())), legend=False)\n",
    "#     plt.title('Validation samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ##============Create dataloader====================\n",
    "    ##Training dataloader\n",
    "    train_batch_size = 50\n",
    "    train_map_data = MapDataset(trainDF, './data/piom_train_png_30k/', transform= transforms.Compose([\n",
    "                                                Rescale(225),\n",
    "                                                CenterCrop((224,224)),\n",
    "                                                Normalize(alpha=0., beta=1.),\n",
    "                                                ToTensor(),\n",
    "                                            ]))\n",
    "    train_loader = torch.utils.data.DataLoader(train_map_data,\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=multiprocessing.cpu_count())\n",
    "    ##Validation dataloader\n",
    "    val_batch_size = 50\n",
    "    validation_map_data = MapDataset(valDF, './data/piom_train2_png_10k/', transform= transforms.Compose([\n",
    "                                                   Rescale(225),\n",
    "                                                   CenterCrop((224,224)),\n",
    "                                                   Normalize(alpha=0., beta=1.),\n",
    "                                                   ToTensor(),\n",
    "                                               ]))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_map_data,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=multiprocessing.cpu_count(),\n",
    "                                              drop_last=False)\n",
    "    print('Training batch size: ', train_batch_size)\n",
    "    print('Validation batch size: ', val_batch_size)\n",
    "    \n",
    "#     ##try data loader\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#         print(i_batch, sample_batch['image'].size())\n",
    "#         if i_batch == 1:\n",
    "#             break\n",
    "            \n",
    "    ##=============Transfer Learning from ResNet34 model for classification==========\n",
    "    model = models.vgg11_bn(pretrained=True, progress=True)\n",
    "    ##modify last fully connected layer\n",
    "#     print(model)\n",
    "    model.fc = nn.Linear(1000,n_zones) #resnet18:512,resnet34:512, resnet34:2048, resnext:2048,\n",
    "    optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "    ###defining the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ###model on GPU if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    #print(model)\n",
    "    \n",
    "    ##============Train the model on geographic zone classification - only the last FC layer====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 3:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    n_epochs = 1\n",
    "    print('============ Training phase 1 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    ##============Train the model on geographic zone classification - all layers====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 3:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "    n_epochs = 5\n",
    "    print('============ Training phase 2 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    \n",
    "#     ##========Drop last FC layer of the model for image embedding=======\n",
    "#     new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "#     ##=================save model in pth file==========================\n",
    "#     MODEL_PATH = './models/gpu_model_resnet18_4_25.pth'\n",
    "#     torch.save(new_classifier.state_dict(), MODEL_PATH)\n",
    "#     print('Model saved!!')\n",
    "    print(\"Training, time: {}h {}min {}s\".format((time.time()-startTime)//3600, \n",
    "                                                                      ((time.time()-startTime)%3600)//60,\n",
    "                                                                      (time.time()-startTime)%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnasnet0_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     startTime = time.time()\n",
    "        \n",
    "# #     # define a seed for reproducibility\n",
    "# #     seed = 5436457\n",
    "# #     torch.manual_seed(seed)\n",
    "    \n",
    "#     ##============Create geographic zones====================\n",
    "#     ###Training set\n",
    "#     trainDF = pd.read_csv(\"./data/piom_train_30k.csv\")\n",
    "#     trainDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "#     lon_dim = 4.\n",
    "#     lat_dim = 2.\n",
    "#     Geo_zones = create_geo_zones(list(trainDF['llcrnrlon']), list(trainDF['llcrnrlat']), lon_dim, lat_dim)\n",
    "#     n_zones = len(Geo_zones)\n",
    "#     print('Number of geographic zones: ', n_zones)\n",
    "#     trainDF['Geo_zone'] = trainDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)    \n",
    "# #     ## plot geographic zones\n",
    "# #     plt.figure(figsize = (15,8))\n",
    "# #     sns.scatterplot(trainDF['llcrnrlon'], trainDF['llcrnrlat'], hue=trainDF['Geo_zone'], \n",
    "# #                     palette=sns.color_palette(\"hls\", n_zones), legend=False)\n",
    "# #     plt.title('Training samples per Geographic Zones')\n",
    "# #     plt.show()\n",
    "# #     ##Visualizing distribution of the geographic zones\n",
    "# #     plt.figure(figsize=(15, 4))\n",
    "# #     trainDF['Geo_zone'].value_counts().plot(kind='bar')\n",
    "# #     plt.ylabel('Count')\n",
    "# #     plt.xlabel('Label')\n",
    "# #     plt.title('Sample distribution per Geographic Zones')\n",
    "# #     plt.show()\n",
    "    \n",
    "#     ###Validation set\n",
    "#     valDF = pd.read_csv(\"./data/piom_train2_10k.csv\")\n",
    "#     valDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "#     valDF['Geo_zone'] = valDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)\n",
    "#     ## plot geographic zones\n",
    "# #     plt.figure(figsize = (15,8))\n",
    "# #     sns.scatterplot(valDF['llcrnrlon'], valDF['llcrnrlat'], hue=valDF['Geo_zone'], \n",
    "# #                     palette=sns.color_palette(\"hls\", len(valDF['Geo_zone'].unique())), legend=False)\n",
    "# #     plt.title('Validation samples per Geographic Zones')\n",
    "# #     plt.show()\n",
    "    \n",
    "#     ##============Create dataloader====================\n",
    "#     ##Training dataloader\n",
    "#     train_batch_size = 50\n",
    "#     train_map_data = MapDataset(trainDF, './data/piom_train_png_30k/', transform= transforms.Compose([\n",
    "#                                                 Rescale(225),\n",
    "#                                                 CenterCrop((224,224)),\n",
    "#                                                 Normalize(alpha=0., beta=1.),\n",
    "#                                                 ToTensor(),\n",
    "#                                             ]))\n",
    "#     train_loader = torch.utils.data.DataLoader(train_map_data,\n",
    "#                                             batch_size=train_batch_size,\n",
    "#                                             shuffle=True,\n",
    "#                                             num_workers=multiprocessing.cpu_count())\n",
    "#     ##Validation dataloader\n",
    "#     val_batch_size = 50\n",
    "#     validation_map_data = MapDataset(valDF, './data/piom_train2_png_10k/', transform= transforms.Compose([\n",
    "#                                                    Rescale(225),\n",
    "#                                                    CenterCrop((224,224)),\n",
    "#                                                    Normalize(alpha=0., beta=1.),\n",
    "#                                                    ToTensor(),\n",
    "#                                                ]))\n",
    "#     validation_loader = torch.utils.data.DataLoader(validation_map_data,\n",
    "#                                               batch_size=val_batch_size,\n",
    "#                                               shuffle=True,\n",
    "#                                               num_workers=multiprocessing.cpu_count(),\n",
    "#                                               drop_last=False)\n",
    "#     print('Training batch size: ', train_batch_size)\n",
    "#     print('Validation batch size: ', val_batch_size)\n",
    "    \n",
    "# #     ##try data loader\n",
    "# #     for i_batch, sample_batch in enumerate(train_loader):\n",
    "# #         print(i_batch, sample_batch['image'].size())\n",
    "# #         if i_batch == 1:\n",
    "# #             break\n",
    "            \n",
    "#     ##=============Transfer Learning from ResNet34 model for classification==========\n",
    "#     model = models.mnasnet0_5(pretrained=True, progress=True)\n",
    "#     ##modify last fully connected layer\n",
    "# #     print(model)\n",
    "#     model.fc = nn.Linear(1000,n_zones) #resnet18:512,resnet34:512, resnet34:2048, resnext:2048,\n",
    "#     optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#     ###defining the loss function\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     ###model on GPU if GPU is available\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     #print(model)\n",
    "    \n",
    "#     ##============Train the model on geographic zone classification - only the last FC layer====================\n",
    "#     ##freeze all layers but the last\n",
    "#     ct = 0\n",
    "#     for child in model.children():\n",
    "#         ct += 1\n",
    "#         if ct < 2:\n",
    "#             for param in child.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#     n_epochs = 1\n",
    "#     print('============ Training phase 1 - number of epochs: {} ============'.format(n_epochs))\n",
    "#     verbose = True\n",
    "#     total_step = len(train_loader)\n",
    "#     loss_list = []\n",
    "#     accuracy_list = []\n",
    "#     compute_validation = True\n",
    "#     val_accuracy_list = []\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for i_batch, sample_batch in enumerate(train_loader):\n",
    "#             # Forward pass\n",
    "#             train_X = sample_batch['image'].float().to(device)\n",
    "#             train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "#             outputs = model(train_X)\n",
    "#             loss = criterion(outputs, train_Y)\n",
    "#             loss_list.append(loss.item())\n",
    "\n",
    "#             # Backprop and optimisation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Track the accuracy\n",
    "#             softmax = torch.exp(outputs).cpu()\n",
    "#             prob = list(softmax.detach().numpy())\n",
    "#             predicted = np.argmax(prob, axis=1)\n",
    "#             accuracy = accuracy_score(predicted,\n",
    "#                                        train_Y.detach().cpu())\n",
    "#             accuracy_list.append(accuracy)\n",
    "\n",
    "#             if verbose:\n",
    "#                 if (i_batch + 1) % 50 == 0:\n",
    "#                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "#                           .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "#         if compute_validation:\n",
    "#             val_predicted = []\n",
    "#             val_true = []\n",
    "#             for i_batch, sample_batch in enumerate(validation_loader):\n",
    "#                 # prediction for validation set\n",
    "#                 with torch.no_grad():\n",
    "#                     val_X = sample_batch['image'].float().to(device)\n",
    "#                     val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "#                     outputs = model(val_X)\n",
    "#                     softmax = torch.exp(outputs).cpu()\n",
    "#                     prob = list(softmax.detach().numpy())\n",
    "#                     predicted = np.argmax(prob, axis=1)\n",
    "#                     val_predicted.extend(predicted)\n",
    "#                 if i_batch == 9:\n",
    "#                     break\n",
    "#             val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "#             val_accuracy_list.append(val_accuracy)\n",
    "#             print('Validation accuracy: {}'.format(val_accuracy))\n",
    "#     ##============Train the model on geographic zone classification - all layers====================\n",
    "#     ##freeze all layers but the last\n",
    "#     ct = 0\n",
    "#     for child in model.children():\n",
    "#         ct += 1\n",
    "#         if ct < 2:\n",
    "#             for param in child.parameters():\n",
    "#                 param.requires_grad = True\n",
    "#     n_epochs = 5\n",
    "#     print('============ Training phase 2 - number of epochs: {} ============'.format(n_epochs))\n",
    "#     verbose = True\n",
    "#     total_step = len(train_loader)\n",
    "#     loss_list = []\n",
    "#     accuracy_list = []\n",
    "#     compute_validation = True\n",
    "#     val_accuracy_list = []\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for i_batch, sample_batch in enumerate(train_loader):\n",
    "#             # Forward pass\n",
    "#             train_X = sample_batch['image'].float().to(device)\n",
    "#             train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "#             outputs = model(train_X)\n",
    "#             loss = criterion(outputs, train_Y)\n",
    "#             loss_list.append(loss.item())\n",
    "\n",
    "#             # Backprop and optimisation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Track the accuracy\n",
    "#             softmax = torch.exp(outputs).cpu()\n",
    "#             prob = list(softmax.detach().numpy())\n",
    "#             predicted = np.argmax(prob, axis=1)\n",
    "#             accuracy = accuracy_score(predicted,\n",
    "#                                        train_Y.detach().cpu())\n",
    "#             accuracy_list.append(accuracy)\n",
    "\n",
    "#             if verbose:\n",
    "#                 if (i_batch + 1) % 50 == 0:\n",
    "#                     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "#                           .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "#         if compute_validation:\n",
    "#             val_predicted = []\n",
    "#             val_true = []\n",
    "#             for i_batch, sample_batch in enumerate(validation_loader):\n",
    "#                 # prediction for validation set\n",
    "#                 with torch.no_grad():\n",
    "#                     val_X = sample_batch['image'].float().to(device)\n",
    "#                     val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "#                     outputs = model(val_X)\n",
    "#                     softmax = torch.exp(outputs).cpu()\n",
    "#                     prob = list(softmax.detach().numpy())\n",
    "#                     predicted = np.argmax(prob, axis=1)\n",
    "#                     val_predicted.extend(predicted)\n",
    "#                 if i_batch == 9:\n",
    "#                     break\n",
    "#             val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "#             val_accuracy_list.append(val_accuracy)\n",
    "#             print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    \n",
    "# #     ##========Drop last FC layer of the model for image embedding=======\n",
    "# #     new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "# #     ##=================save model in pth file==========================\n",
    "# #     MODEL_PATH = './models/gpu_model_resnet18_4_25.pth'\n",
    "# #     torch.save(new_classifier.state_dict(), MODEL_PATH)\n",
    "# #     print('Model saved!!')\n",
    "#     print(\"Training, time: {}h {}min {}s\".format((time.time()-startTime)//3600, \n",
    "#                                                                       ((time.time()-startTime)%3600)//60,\n",
    "#                                                                       (time.time()-startTime)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\" to C:\\Users\\qc16/.cache\\torch\\checkpoints\\shufflenetv2_x0.5-f707e7126e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e34e373edd4f7691958e886861bb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5538128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ShuffleNetV2(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (stage2): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (branch1): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU(inplace=True)\n",
      "      )\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage3): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (branch1): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU(inplace=True)\n",
      "      )\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage4): Sequential(\n",
      "    (0): InvertedResidual(\n",
      "      (branch1): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU(inplace=True)\n",
      "      )\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (branch1): Sequential()\n",
      "      (branch2): Sequential(\n",
      "        (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (7): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(192, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
      ")\n",
      "number of chidren:  7\n"
     ]
    }
   ],
   "source": [
    "model = models.shufflenet_v2_x0_5(pretrained=True, progress=True)\n",
    "##modify last fully connected layer\n",
    "# model.fc = nn.Linear(1000,12)\n",
    "print(model)\n",
    "ct = 0\n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "print('number of chidren: ', ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of geographic zones:  676\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Out of Zone\n",
      "Training batch size:  50\n",
      "Validation batch size:  50\n",
      "============ Training phase 1 - number of epochs: 1 ============\n",
      "Epoch [1/1], Step [50/600], Loss: 6.4327, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [100/600], Loss: 6.4193, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [150/600], Loss: 6.3251, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [200/600], Loss: 6.2765, Accuracy: 0.04%\n",
      "Epoch [1/1], Step [250/600], Loss: 6.2504, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [300/600], Loss: 6.2451, Accuracy: 0.02%\n",
      "Epoch [1/1], Step [350/600], Loss: 6.2710, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [400/600], Loss: 6.2186, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [450/600], Loss: 6.1874, Accuracy: 0.02%\n",
      "Epoch [1/1], Step [500/600], Loss: 6.2688, Accuracy: 0.00%\n",
      "Epoch [1/1], Step [550/600], Loss: 6.2675, Accuracy: 0.02%\n",
      "Epoch [1/1], Step [600/600], Loss: 6.1687, Accuracy: 0.00%\n",
      "Validation accuracy: 0.012\n",
      "============ Training phase 2 - number of epochs: 10 ============\n",
      "Epoch [1/10], Step [50/600], Loss: 6.1722, Accuracy: 0.00%\n",
      "Epoch [1/10], Step [100/600], Loss: 5.9830, Accuracy: 0.04%\n",
      "Epoch [1/10], Step [150/600], Loss: 5.7776, Accuracy: 0.02%\n",
      "Epoch [1/10], Step [200/600], Loss: 5.1620, Accuracy: 0.12%\n",
      "Epoch [1/10], Step [250/600], Loss: 5.0980, Accuracy: 0.10%\n",
      "Epoch [1/10], Step [300/600], Loss: 4.9041, Accuracy: 0.08%\n",
      "Epoch [1/10], Step [350/600], Loss: 4.7862, Accuracy: 0.14%\n",
      "Epoch [1/10], Step [400/600], Loss: 5.1367, Accuracy: 0.06%\n",
      "Epoch [1/10], Step [450/600], Loss: 4.8191, Accuracy: 0.10%\n",
      "Epoch [1/10], Step [500/600], Loss: 3.8921, Accuracy: 0.28%\n",
      "Epoch [1/10], Step [550/600], Loss: 3.7864, Accuracy: 0.28%\n",
      "Epoch [1/10], Step [600/600], Loss: 4.6432, Accuracy: 0.18%\n",
      "Validation accuracy: 0.266\n",
      "Epoch [2/10], Step [50/600], Loss: 3.9228, Accuracy: 0.26%\n",
      "Epoch [2/10], Step [100/600], Loss: 3.4536, Accuracy: 0.30%\n",
      "Epoch [2/10], Step [150/600], Loss: 3.5161, Accuracy: 0.38%\n",
      "Epoch [2/10], Step [200/600], Loss: 3.4489, Accuracy: 0.38%\n",
      "Epoch [2/10], Step [250/600], Loss: 3.6082, Accuracy: 0.32%\n",
      "Epoch [2/10], Step [300/600], Loss: 2.8360, Accuracy: 0.50%\n",
      "Epoch [2/10], Step [350/600], Loss: 3.3588, Accuracy: 0.42%\n",
      "Epoch [2/10], Step [400/600], Loss: 2.8733, Accuracy: 0.44%\n",
      "Epoch [2/10], Step [450/600], Loss: 2.3539, Accuracy: 0.60%\n",
      "Epoch [2/10], Step [500/600], Loss: 2.8093, Accuracy: 0.52%\n",
      "Epoch [2/10], Step [550/600], Loss: 2.8158, Accuracy: 0.40%\n",
      "Epoch [2/10], Step [600/600], Loss: 3.0806, Accuracy: 0.36%\n",
      "Validation accuracy: 0.496\n",
      "Epoch [3/10], Step [50/600], Loss: 2.8267, Accuracy: 0.36%\n",
      "Epoch [3/10], Step [100/600], Loss: 2.5439, Accuracy: 0.42%\n",
      "Epoch [3/10], Step [150/600], Loss: 2.3453, Accuracy: 0.46%\n",
      "Epoch [3/10], Step [200/600], Loss: 2.0790, Accuracy: 0.44%\n",
      "Epoch [3/10], Step [250/600], Loss: 2.1517, Accuracy: 0.48%\n",
      "Epoch [3/10], Step [300/600], Loss: 2.1658, Accuracy: 0.52%\n",
      "Epoch [3/10], Step [350/600], Loss: 2.3817, Accuracy: 0.50%\n",
      "Epoch [3/10], Step [400/600], Loss: 1.7874, Accuracy: 0.58%\n",
      "Epoch [3/10], Step [450/600], Loss: 2.3162, Accuracy: 0.50%\n",
      "Epoch [3/10], Step [500/600], Loss: 1.7052, Accuracy: 0.64%\n",
      "Epoch [3/10], Step [550/600], Loss: 1.7973, Accuracy: 0.46%\n",
      "Epoch [3/10], Step [600/600], Loss: 1.6848, Accuracy: 0.60%\n",
      "Validation accuracy: 0.58\n",
      "Epoch [4/10], Step [50/600], Loss: 1.3879, Accuracy: 0.62%\n",
      "Epoch [4/10], Step [100/600], Loss: 2.0325, Accuracy: 0.50%\n",
      "Epoch [4/10], Step [150/600], Loss: 1.6828, Accuracy: 0.66%\n",
      "Epoch [4/10], Step [200/600], Loss: 1.5621, Accuracy: 0.58%\n",
      "Epoch [4/10], Step [250/600], Loss: 1.3286, Accuracy: 0.68%\n",
      "Epoch [4/10], Step [300/600], Loss: 1.6081, Accuracy: 0.66%\n",
      "Epoch [4/10], Step [350/600], Loss: 1.6307, Accuracy: 0.56%\n",
      "Epoch [4/10], Step [400/600], Loss: 1.9468, Accuracy: 0.58%\n",
      "Epoch [4/10], Step [450/600], Loss: 1.1098, Accuracy: 0.70%\n",
      "Epoch [4/10], Step [500/600], Loss: 1.3985, Accuracy: 0.64%\n",
      "Epoch [4/10], Step [550/600], Loss: 1.2417, Accuracy: 0.60%\n",
      "Epoch [4/10], Step [600/600], Loss: 1.3154, Accuracy: 0.60%\n",
      "Validation accuracy: 0.6\n",
      "Epoch [5/10], Step [50/600], Loss: 1.1983, Accuracy: 0.68%\n",
      "Epoch [5/10], Step [100/600], Loss: 1.4299, Accuracy: 0.64%\n",
      "Epoch [5/10], Step [150/600], Loss: 1.2106, Accuracy: 0.72%\n",
      "Epoch [5/10], Step [200/600], Loss: 1.1054, Accuracy: 0.68%\n",
      "Epoch [5/10], Step [250/600], Loss: 0.8639, Accuracy: 0.78%\n",
      "Epoch [5/10], Step [300/600], Loss: 1.2408, Accuracy: 0.74%\n",
      "Epoch [5/10], Step [350/600], Loss: 1.3294, Accuracy: 0.62%\n",
      "Epoch [5/10], Step [400/600], Loss: 1.8170, Accuracy: 0.56%\n",
      "Epoch [5/10], Step [450/600], Loss: 1.4536, Accuracy: 0.64%\n",
      "Epoch [5/10], Step [500/600], Loss: 0.9647, Accuracy: 0.68%\n",
      "Epoch [5/10], Step [550/600], Loss: 1.6972, Accuracy: 0.54%\n",
      "Epoch [5/10], Step [600/600], Loss: 1.1232, Accuracy: 0.64%\n",
      "Validation accuracy: 0.648\n",
      "Epoch [6/10], Step [50/600], Loss: 1.2089, Accuracy: 0.76%\n",
      "Epoch [6/10], Step [100/600], Loss: 1.8782, Accuracy: 0.62%\n",
      "Epoch [6/10], Step [150/600], Loss: 1.1689, Accuracy: 0.66%\n",
      "Epoch [6/10], Step [200/600], Loss: 0.9956, Accuracy: 0.70%\n",
      "Epoch [6/10], Step [250/600], Loss: 0.9207, Accuracy: 0.76%\n",
      "Epoch [6/10], Step [300/600], Loss: 0.9805, Accuracy: 0.78%\n",
      "Epoch [6/10], Step [350/600], Loss: 0.8302, Accuracy: 0.80%\n",
      "Epoch [6/10], Step [400/600], Loss: 1.0613, Accuracy: 0.78%\n",
      "Epoch [6/10], Step [450/600], Loss: 1.3262, Accuracy: 0.68%\n",
      "Epoch [6/10], Step [500/600], Loss: 1.0787, Accuracy: 0.74%\n",
      "Epoch [6/10], Step [550/600], Loss: 0.9431, Accuracy: 0.72%\n",
      "Epoch [6/10], Step [600/600], Loss: 0.9964, Accuracy: 0.72%\n",
      "Validation accuracy: 0.68\n",
      "Epoch [7/10], Step [50/600], Loss: 0.7826, Accuracy: 0.76%\n",
      "Epoch [7/10], Step [100/600], Loss: 1.1912, Accuracy: 0.74%\n",
      "Epoch [7/10], Step [150/600], Loss: 0.6913, Accuracy: 0.80%\n",
      "Epoch [7/10], Step [200/600], Loss: 0.8655, Accuracy: 0.78%\n",
      "Epoch [7/10], Step [250/600], Loss: 1.1031, Accuracy: 0.62%\n",
      "Epoch [7/10], Step [300/600], Loss: 0.8927, Accuracy: 0.78%\n",
      "Epoch [7/10], Step [350/600], Loss: 1.0351, Accuracy: 0.68%\n",
      "Epoch [7/10], Step [400/600], Loss: 0.9939, Accuracy: 0.72%\n",
      "Epoch [7/10], Step [450/600], Loss: 0.8029, Accuracy: 0.76%\n",
      "Epoch [7/10], Step [500/600], Loss: 1.0346, Accuracy: 0.68%\n",
      "Epoch [7/10], Step [550/600], Loss: 0.9788, Accuracy: 0.68%\n",
      "Epoch [7/10], Step [600/600], Loss: 0.5687, Accuracy: 0.84%\n",
      "Validation accuracy: 0.71\n",
      "Epoch [8/10], Step [50/600], Loss: 0.7659, Accuracy: 0.84%\n",
      "Epoch [8/10], Step [100/600], Loss: 0.7498, Accuracy: 0.72%\n",
      "Epoch [8/10], Step [150/600], Loss: 0.8421, Accuracy: 0.72%\n",
      "Epoch [8/10], Step [200/600], Loss: 0.7499, Accuracy: 0.78%\n",
      "Epoch [8/10], Step [250/600], Loss: 0.9025, Accuracy: 0.78%\n",
      "Epoch [8/10], Step [300/600], Loss: 0.9010, Accuracy: 0.78%\n",
      "Epoch [8/10], Step [350/600], Loss: 0.6596, Accuracy: 0.82%\n",
      "Epoch [8/10], Step [400/600], Loss: 0.9566, Accuracy: 0.72%\n",
      "Epoch [8/10], Step [450/600], Loss: 0.7976, Accuracy: 0.76%\n",
      "Epoch [8/10], Step [500/600], Loss: 1.0612, Accuracy: 0.72%\n",
      "Epoch [8/10], Step [550/600], Loss: 0.8512, Accuracy: 0.78%\n",
      "Epoch [8/10], Step [600/600], Loss: 0.9721, Accuracy: 0.76%\n",
      "Validation accuracy: 0.698\n",
      "Epoch [9/10], Step [50/600], Loss: 0.8118, Accuracy: 0.78%\n",
      "Epoch [9/10], Step [100/600], Loss: 0.7851, Accuracy: 0.80%\n",
      "Epoch [9/10], Step [150/600], Loss: 1.0325, Accuracy: 0.72%\n",
      "Epoch [9/10], Step [200/600], Loss: 0.9368, Accuracy: 0.78%\n",
      "Epoch [9/10], Step [250/600], Loss: 0.8367, Accuracy: 0.82%\n",
      "Epoch [9/10], Step [300/600], Loss: 0.7976, Accuracy: 0.82%\n",
      "Epoch [9/10], Step [350/600], Loss: 0.8480, Accuracy: 0.78%\n",
      "Epoch [9/10], Step [400/600], Loss: 0.6794, Accuracy: 0.84%\n",
      "Epoch [9/10], Step [450/600], Loss: 0.6513, Accuracy: 0.80%\n",
      "Epoch [9/10], Step [500/600], Loss: 0.6311, Accuracy: 0.82%\n",
      "Epoch [9/10], Step [550/600], Loss: 0.6847, Accuracy: 0.80%\n",
      "Epoch [9/10], Step [600/600], Loss: 0.7227, Accuracy: 0.72%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7863977c3977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mval_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mval_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;31m# prediction for validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\map_env\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    startTime = time.time()\n",
    "        \n",
    "#     # define a seed for reproducibility\n",
    "#     seed = 5436457\n",
    "#     torch.manual_seed(seed)\n",
    "    \n",
    "    ##============Create geographic zones====================\n",
    "    ###Training set\n",
    "    trainDF = pd.read_csv(\"./data/piom_train_30k.csv\")\n",
    "    trainDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    lon_dim = 4.\n",
    "    lat_dim = 2.\n",
    "    Geo_zones = create_geo_zones(list(trainDF['llcrnrlon']), list(trainDF['llcrnrlat']), lon_dim, lat_dim)\n",
    "    n_zones = len(Geo_zones)\n",
    "    print('Number of geographic zones: ', n_zones)\n",
    "    trainDF['Geo_zone'] = trainDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)    \n",
    "#     ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(trainDF['llcrnrlon'], trainDF['llcrnrlat'], hue=trainDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", n_zones), legend=False)\n",
    "#     plt.title('Training samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "#     ##Visualizing distribution of the geographic zones\n",
    "#     plt.figure(figsize=(15, 4))\n",
    "#     trainDF['Geo_zone'].value_counts().plot(kind='bar')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xlabel('Label')\n",
    "#     plt.title('Sample distribution per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ###Validation set\n",
    "    valDF = pd.read_csv(\"./data/piom_train2_10k.csv\")\n",
    "    valDF.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "    valDF['Geo_zone'] = valDF.apply(lambda row: classify_geographic_zones(Geo_zones, row, lon_dim, lat_dim), axis=1)\n",
    "    ## plot geographic zones\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     sns.scatterplot(valDF['llcrnrlon'], valDF['llcrnrlat'], hue=valDF['Geo_zone'], \n",
    "#                     palette=sns.color_palette(\"hls\", len(valDF['Geo_zone'].unique())), legend=False)\n",
    "#     plt.title('Validation samples per Geographic Zones')\n",
    "#     plt.show()\n",
    "    \n",
    "    ##============Create dataloader====================\n",
    "    ##Training dataloader\n",
    "    train_batch_size = 50\n",
    "    train_map_data = MapDataset(trainDF, './data/piom_train_png_30k/', transform= transforms.Compose([\n",
    "                                                Rescale(225),\n",
    "                                                CenterCrop((224,224)),\n",
    "                                                Normalize(alpha=0., beta=1.),\n",
    "                                                ToTensor(),\n",
    "                                            ]))\n",
    "    train_loader = torch.utils.data.DataLoader(train_map_data,\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=multiprocessing.cpu_count())\n",
    "    ##Validation dataloader\n",
    "    val_batch_size = 50\n",
    "    validation_map_data = MapDataset(valDF, './data/piom_train2_png_10k/', transform= transforms.Compose([\n",
    "                                                   Rescale(225),\n",
    "                                                   CenterCrop((224,224)),\n",
    "                                                   Normalize(alpha=0., beta=1.),\n",
    "                                                   ToTensor(),\n",
    "                                               ]))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_map_data,\n",
    "                                              batch_size=val_batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=multiprocessing.cpu_count(),\n",
    "                                              drop_last=False)\n",
    "    print('Training batch size: ', train_batch_size)\n",
    "    print('Validation batch size: ', val_batch_size)\n",
    "    \n",
    "#     ##try data loader\n",
    "#     for i_batch, sample_batch in enumerate(train_loader):\n",
    "#         print(i_batch, sample_batch['image'].size())\n",
    "#         if i_batch == 1:\n",
    "#             break\n",
    "            \n",
    "    ##=============Transfer Learning from ResNet34 model for classification==========\n",
    "    model = models.shufflenet_v2_x0_5(pretrained=True, progress=True)\n",
    "    ##modify last fully connected layer\n",
    "#     print(model)\n",
    "#     model.classifier[1] = nn.Conv2d(512, n_zones, kernel_size=(1,1), stride=(1,1))\n",
    "    model.fc = nn.Linear(1024,n_zones) #resnet18:512,resnet34:512, resnet34:2048, resnext:2048,\n",
    "    # change the internal num_classes variable rather than redefining the forward pass\n",
    "    model.num_classes = n_zones\n",
    "    optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "    ###defining the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ###model on GPU if GPU is available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    #print(model)\n",
    "    \n",
    "    ##============Train the model on geographic zone classification - only the last FC layer====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 7:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    n_epochs = 1\n",
    "    print('============ Training phase 1 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    ##============Train the model on geographic zone classification - all layers====================\n",
    "    ##freeze all layers but the last\n",
    "    ct = 0\n",
    "    for child in model.children():\n",
    "        ct += 1\n",
    "        if ct < 7:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "    n_epochs = 10\n",
    "    print('============ Training phase 2 - number of epochs: {} ============'.format(n_epochs))\n",
    "    verbose = True\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    compute_validation = True\n",
    "    val_accuracy_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            train_X = sample_batch['image'].float().to(device)\n",
    "            train_Y = sample_batch['Geo_zone'].long().to(device)\n",
    "            outputs = model(train_X)\n",
    "            loss = criterion(outputs, train_Y)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            softmax = torch.exp(outputs).cpu()\n",
    "            prob = list(softmax.detach().numpy())\n",
    "            predicted = np.argmax(prob, axis=1)\n",
    "            accuracy = accuracy_score(predicted,\n",
    "                                       train_Y.detach().cpu())\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                if (i_batch + 1) % 50 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i_batch + 1, total_step, loss.item(), accuracy))\n",
    "        if compute_validation:\n",
    "            val_predicted = []\n",
    "            val_true = []\n",
    "            for i_batch, sample_batch in enumerate(validation_loader):\n",
    "                # prediction for validation set\n",
    "                with torch.no_grad():\n",
    "                    val_X = sample_batch['image'].float().to(device)\n",
    "                    val_true.extend(sample_batch['Geo_zone'].long().detach().cpu())\n",
    "                    outputs = model(val_X)\n",
    "                    softmax = torch.exp(outputs).cpu()\n",
    "                    prob = list(softmax.detach().numpy())\n",
    "                    predicted = np.argmax(prob, axis=1)\n",
    "                    val_predicted.extend(predicted)\n",
    "                if i_batch == 9:\n",
    "                    break\n",
    "            val_accuracy = accuracy_score(val_predicted, torch.stack(val_true))\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print('Validation accuracy: {}'.format(val_accuracy))\n",
    "    \n",
    "#     ##========Drop last FC layer of the model for image embedding=======\n",
    "#     new_classifier = nn.Sequential(*list(model.children())[:-1])\n",
    "    \n",
    "#     ##=================save model in pth file==========================\n",
    "#     MODEL_PATH = './models/gpu_model_resnet18_4_25.pth'\n",
    "#     torch.save(new_classifier.state_dict(), MODEL_PATH)\n",
    "#     print('Model saved!!')\n",
    "    print(\"Training, time: {}h {}min {}s\".format((time.time()-startTime)//3600, \n",
    "                                                  ((time.time()-startTime)%3600)//60,\n",
    "                                                  (time.time()-startTime)%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map_env",
   "language": "python",
   "name": "map_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
